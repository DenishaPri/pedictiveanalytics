# -*- coding: utf-8 -*-
"""LVADSUSR81_DENISHAPRIYAG_LAB1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11-hwkKg9Pua-cHwHv1lSwPRBU0bRJM4A
"""

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import r2_score, mean_squared_error, accuracy_score

df = pd.read_csv('/content/drive/MyDrive/expenses.csv')
# print(df)
df.columns.tolist()

# -------------Handle missing values------------
if df.isna().sum().sum()>0:
  df.dropna(inplace=True)

# ----------outliers---------------
X = df.drop(['sex','smoker', 'region','charges'], axis=1)
x = df.drop('charges', axis=1)
y = df['charges']
print('Before removing outliers')
print()
print()
#creating a box plot
for i in X:
  plt.figure(figsize = (5,5))
  sns.boxplot(data=X[i])
  plt.title(i)
  plt.ylabel('Values')
  plt.xticks(rotation = 45)
  plt.show()
#replace outlier with median
def detect_and_treat_outliers(df,columns):

  for col in columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    #define bounds for outliers
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    #replace outliers with the median of column
    median = df[col].median()
    df[col] = np.where((df[col] < lower_bound) | (df[col] > upper_bound), median, df[col])

  return df

df = detect_and_treat_outliers(df,X)
# print(df)
X = df.drop(['sex','smoker', 'region','charges'], axis=1)
x = df.drop('charges', axis=1)
y = df['charges']
print('After removing outliers')
print()
print()
#creating a box plot after removing outliers
for i in X:
  plt.figure(figsize = (5,5))
  sns.boxplot(data=X[i])
  plt.title(i)
  plt.ylabel('Values')
  plt.xticks(rotation = 45)
  plt.show()

# ---------------Encoding categorical data---------

label_encoder = LabelEncoder()
df['sex'] = label_encoder.fit_transform(df['sex'])
df['smoker'] = label_encoder.fit_transform(df['smoker'])
df[ 'region'] = label_encoder.fit_transform(df['region'])

#------------------ Feature selection-------------
x = df.drop('charges', axis=1)
y = df['charges']
print(x)
#---------------- data cleaning---------------
if df.duplicated().sum()>0:
  df.drop_duplicates(inplace = True)

# ---------------Splitting the dataset into training and testing sets-----------

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

linear_model = LinearRegression()
clf = linear_model.fit(x_train, y_train)

pred = linear_model.predict(x_test)
print('Predicted: ',pred)
print('Test: ',y_test)

"""--- 5

Learning Rate:
The learning rate  controls the size of the steps taken during gradient descent.Like shifting between models. From model 1 to model 2. Less learning rate leads to slower convergence larger learning rate have more convergence but may cause divergence.

Derivatives and Partial Derivatives:
Partial derivatives for multiple variables. They are used to compute the gradient of the cost function with respect to the model parameters. Negative gradient points in the direction of the steepest decrease.

"""

# ---------Calculate evaluation metrics-----------

r2 = r2_score(y_test, pred)
rmse = np.sqrt(mean_squared_error(y_test,pred))

print("R-squared:", r2)
print("Root Mean Squared Error (RMSE):", rmse)